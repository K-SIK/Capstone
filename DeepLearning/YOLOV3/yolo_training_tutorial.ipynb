{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b87ba9",
   "metadata": {},
   "source": [
    "# YOLO Training Tutorial\n",
    "\n",
    "In this notebook, I will talk about training process using VOC 2012 dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb2780",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "1. Able to detect image using pretrained darknet model\n",
    "2. Many Gigabytes of Disk Space\n",
    "3. High Speed Internet Connection Preferred\n",
    "4. GPU Preferred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756706c8",
   "metadata": {},
   "source": [
    "### 1. Download Dataset\n",
    "\n",
    "You can read the full description of VOC dataset [here](http://host.robots.ox.ac.uk/pascal/VOC/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3620807c",
   "metadata": {},
   "source": [
    "#### In command, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c7bda",
   "metadata": {},
   "source": [
    "먼저, VOC 데이터셋 공식 사이트에서 훈련/검증 데이터셋을 data/ 에 다운로드 받습니다. \n",
    "\n",
    "> wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar -O ./data/voc2012_raw.tar\n",
    "\n",
    "만약 Windows 운영체제를 사용한다면, wget 설치 파일을 시스템 폴더에 포함시켜서 wget 명령어를 사용하거나, 사이트에서 직접 다운로드합니다. \n",
    "\n",
    "윈도우에서 wget 명령어를 사용하는 방법은 [이 블로그](https://sound10000w.tistory.com/229)를 참조하세요. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0905692",
   "metadata": {},
   "source": [
    "다운로드 받은 VOC 2012 데이터셋은 tar 파일로 압축되어 있습니다. 압축을 푼 데이터들을 저장할 디렉터리를 하나 만들어줍니다. \n",
    "\n",
    "> mkdir -p ./data/voc2012_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97318fd",
   "metadata": {},
   "source": [
    "tar 파일을 앞서 만든 디렉터리에 압축 해제합니다. \n",
    "\n",
    "> tar -xf ./data/voc2012_raw.tar -C ./data/voc2012_raw\n",
    "\n",
    "만약 Windows 운영체제를 사용한다면, tar 압축/압축 해제 용 프로그램을 설치해야 합니다. \n",
    "\n",
    "설치 및 압축/압축해제 과정은 [이 블로그](https://www.itopening.com/708/)를 참조하세요. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a4a93",
   "metadata": {},
   "source": [
    "데이터셋을 한 번 보겠습니다. \n",
    "\n",
    "> ls ./data/voc2012_raw/VOCdevkit/VOC2012 \n",
    "\n",
    "파이썬 코드로는 아래와 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c9a85ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations\n",
      "ImageSets\n",
      "JPEGImages\n",
      "SegmentationClass\n",
      "SegmentationObject\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = './data/voc2012_raw/VOCdevkit/VOC2012'\n",
    "file_list = os.listdir(path)\n",
    "\n",
    "print(*file_list, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cf0912",
   "metadata": {},
   "source": [
    "### 2. Transform Dataset\n",
    "\n",
    "See tools/voc2012.py for implementation, this format is based on [tensorflow object detection API](https://github.com/tensorflow/models/tree/master/research/object_detection). \n",
    "\n",
    "Many fields are not required, I left them there for compatibility with official API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c08df4",
   "metadata": {},
   "source": [
    "#### In command, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b6ac8",
   "metadata": {},
   "source": [
    "> python tools/voc2012.py \\\n",
    "  --data_dir ./data/voc2012_raw/VOCdevkit/VOC2012 \\\n",
    "  --split train \\\n",
    "  --output_file ./data/voc2012_train.tfrecord\n",
    "  \n",
    "> python tools/voc2012.py \\\n",
    "  --data_dir ./data/voc2012_raw/VOCdevkit/VOC2012 \\\n",
    "  --split val \\\n",
    "  --output_file ./data/voc2012_val.tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3924c",
   "metadata": {},
   "source": [
    "아래 명령을 통해 데이터셋을 visualize할 수 있습니다. \n",
    "\n",
    "> python tools/visualize_dataset.py --classes ./data/voc2012.names\n",
    "\n",
    "위 명령을 커맨드에서 실행하면 데이터셋 중 하나의 이미지를 랜덤으로 output.jpg로 저장합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2223160b",
   "metadata": {},
   "source": [
    "### 3. Training\n",
    "\n",
    "You can adjust the parameters based on your setup\n",
    "\n",
    "> python train.py \\\n",
    "\t--dataset ./data/voc2012_train.tfrecord \\\n",
    "\t--val_dataset ./data/voc2012_val.tfrecord \\\n",
    "\t--classes ./data/voc2012.names \\\n",
    "\t--num_classes 20 \\\n",
    "\t--mode fit \n",
    "    --transfer darknet \\\n",
    "\t--batch_size 16 \\\n",
    "\t--epochs 10 \\\n",
    "\t--weights ./checkpoints/yolov3.tf \\\n",
    "\t--weights_num_classes 80 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cba6f1",
   "metadata": {},
   "source": [
    "python train.py \\\n",
    "    --dataset E:/Capstone_largefiles/DeepLearning/YOLOV3/data/koreanfood_train.tfrecord \\\n",
    "    --val_dataset E:/Capstone_largefiles/DeepLearning/YOLOV3/data/koreanfood_val.tfrecord \\\n",
    "    --weights E:/Capstone_largefiles/DeepLearning/YOLOV3/checkpoints/yolov3.tf \\\n",
    "    --classes E:/Capstone_largefiles/DeepLearning/YOLOV3/data/images/koreanfood/koreanfood.names \\\n",
    "    --num_classes 400 \\\n",
    "    --mode fit \\\n",
    "    --transfer darknet \\\n",
    "    --epochs 2 \\\n",
    "    --batch_size 8 \\\n",
    "    --weights_num_classes 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771894dd",
   "metadata": {},
   "source": [
    "I have tested this works 100% with correct loss and converging over time.\n",
    "Each epoch takes around 10 minutes on single AWS p2.xlarge (Nvidia K80 GPU) Instance.\n",
    "\n",
    "You might see warnings or error messages during training, they are not critical dont' worry too much about them.\n",
    "There might be a long wait time between each epoch becaues we are calculating validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a9418e",
   "metadata": {},
   "source": [
    "#### 4. Inference\n",
    "\n",
    "**detect from images**\n",
    "\n",
    "> python detect.py \\\n",
    "\t--classes ./data/voc2012.names \\\n",
    "\t--num_classes 20 \\\n",
    "\t--weights ./checkpoints/yolov3_train_5.tf \\\n",
    "\t--image ./data/street.jpg\n",
    "\n",
    "**detect from validation set**\n",
    "\n",
    "> python detect.py \\\n",
    "\t--classes ./data/voc2012.names \\\n",
    "\t--num_classes 20 \\\n",
    "\t--weights ./checkpoints/yolov3_train_5.tf \\\n",
    "\t--tfrecord ./data/voc2012_val.tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d365cc58",
   "metadata": {},
   "source": [
    "You should see some detect objects in the standard output and the visualization at `output.jpg`.\n",
    "this is just a proof of concept, so it won't be as good as pretrained models.\n",
    "In my experience, you might need lower score score thershold if you didn't train it enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feda8391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
